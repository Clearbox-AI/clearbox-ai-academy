{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dae1932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7f71dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I am fine, thank you!\",\n",
    "    \"Let's create a custom tokenizer.\",\n",
    "    \"Tokenization is fun and useful.\",\n",
    "    \"This is an example of training a tokenizer from scratch.\",\n",
    "    \"Machine learning can be applied in many fields.\",\n",
    "    \"Natural language processing involves analyzing text data.\",\n",
    "    \"Deep learning models understand complex patterns in data.\",\n",
    "    \"Data science is an interdisciplinary field combining statistics, computer science, and domain expertise.\",\n",
    "    \"Python is a versatile and powerful programming language.\",\n",
    "    \"Jupyter Notebooks are great for interactive and exploratory coding.\",\n",
    "    \"Artificial intelligence is transforming various industries.\",\n",
    "    \"Open source libraries accelerate innovation in technology.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Building a tokenizer requires careful text preprocessing and experimentation.\",\n",
    "    \"We can add special tokens like <im_user> to highlight specific parts of the text.\",\n",
    "    \"Tokenizers enable efficient text representation for language models.\",\n",
    "    \"Clean and diverse datasets are crucial for training robust models.\",\n",
    "    \"Experimentation and iteration are key to success in machine learning projects.\",\n",
    "    \"Always test your models with new and unseen data to ensure generalization.\",\n",
    "    \"Custom tokenization can help tailor the model to your specific dataset.\",\n",
    "    \"Data augmentation techniques improve the performance of machine learning models.\",\n",
    "    \"Learning how to tokenize text is a fundamental skill in natural language processing.\",\n",
    "    \"Preprocessing text data helps remove noise and irrelevant characters.\",\n",
    "    \"A well-trained tokenizer forms the basis for many language processing tasks.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5746af7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "special_tokens = [\"<pad>\", \"<unk>\", \"<im_user>\"]\n",
    "\n",
    "trainer = trainers.BpeTrainer(vocab_size=1000, special_tokens=special_tokens)\n",
    "\n",
    "tokenizer.train_from_iterator(texts, trainer=trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6dfca31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [478, 97, 68, 448, 442, 104]\n",
      "Tokens: ['This', 'Ġis', 'Ġan', 'Ġunseen', 'Ġnew', 'Ġtext']\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"This is an unseen new text\"\n",
    "encoding = tokenizer.encode(sample_text)\n",
    "\n",
    "print(\"Token IDs:\", encoding.ids)\n",
    "print(\"Tokens:\", encoding.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6110b92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"custom_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f2ea2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer tokens: ['This', 'Ġis', 'Ġan', 'Ġunseen', 'Ġnew', 'Ġtext']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "loaded_tokenizer = Tokenizer.from_file(\"custom_tokenizer.json\")\n",
    "\n",
    "loaded_encoding = loaded_tokenizer.encode(sample_text)\n",
    "print(\"Loaded tokenizer tokens:\", loaded_encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35ae0250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./llm.c/dev/data/tinystories/TinyStories_all_data/data49.json\", \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "print(json_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e87d5794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'story': 'Once upon a time, in a small house, there lived a little girl named Sue. Sue had a pretty scarf. She loved her scarf very much. It was soft and gentle. She wore it every day.\\nOne day, her friend Tim came to play. Tim saw the scarf and liked it too. He said, \"Please, can I wear the scarf too?\" Sue was happy to share. She said, \"Yes, please be gentle with my scarf.\"\\nThey played all day with the scarf. They ran, they jumped, and they laughed. They were very careful and gentle with the scarf. At the end of the day, Tim gave the scarf back to Sue. He said, \"Thank you for sharing your scarf with me.\"\\nSue smiled and said, \"You\\'re welcome. I\\'m glad we could share my scarf and have fun together.\" They hugged and said goodbye. Sue went inside her house, still wearing her gentle scarf, feeling happy and warm.',\n",
       " 'instruction': {'prompt:': 'Write a short story (3-5 paragraphs) which only uses very simple words that a 3 year old child would understand. The story should use the verb \"please\", the noun \"scarf\" and the adjective \"gentle\". Remember to only use simple words!',\n",
       "  'words': ['please', 'scarf', 'gentle'],\n",
       "  'features': []},\n",
       " 'summary': 'Sue shares her beloved scarf with her friend Tim, who is careful and gentle with it, and they have a fun day playing together before Tim returns the scarf to Sue.',\n",
       " 'source': 'GPT-4'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "013ad7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magic Number: 20240520\n",
      "Version: 1\n",
      "Number of Tokens: 32768\n",
      "Loaded 32,768 tokens\n",
      "[50256  5962 22307    25   198  8421   356  5120   597  2252]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_datafile(filename, model_desc=\"gpt-2\"):\n",
    "    \"\"\"\n",
    "    Reads a binary token file written by `write_datafile()`.\n",
    "    \"\"\"\n",
    "    info = {\n",
    "        \"gpt-2\": {\"token_dtype\": np.uint16},  # 2 bytes per token\n",
    "        \"llama-3\": {\"token_dtype\": np.uint32} # 4 bytes per token\n",
    "    }\n",
    "    \n",
    "    assert model_desc in info, f\"Unknown model descriptor {model_desc}\"\n",
    "    \n",
    "    with open(filename, \"rb\") as f:\n",
    "        header = np.frombuffer(f.read(1024), dtype=np.int32)\n",
    "        \n",
    "        magic_number = header[0]\n",
    "        version = header[1]\n",
    "        num_tokens = header[2]\n",
    "\n",
    "        print(f\"Magic Number: {magic_number}\")\n",
    "        print(f\"Version: {version}\")\n",
    "        print(f\"Number of Tokens: {num_tokens}\")\n",
    "\n",
    "        token_dtype = info[model_desc][\"token_dtype\"]\n",
    "        tokens = np.frombuffer(f.read(num_tokens * np.dtype(token_dtype).itemsize), dtype=token_dtype)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "tokens = read_datafile(\"./llm.c/dev/data/tinyshakespeare/tiny_shakespeare_val.bin\", model_desc=\"gpt-2\")  # or \"llama-3\"\n",
    "print(f\"Loaded {len(tokens):,} tokens\")\n",
    "print(tokens[:10]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a864e863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a156c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2314f677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
